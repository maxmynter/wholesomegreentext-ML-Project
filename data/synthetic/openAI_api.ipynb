{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGreentexts on 4chan cover a wide range of topics, reflecting the diverse interests and experiences of its user base.\\nThese topics can range from the mundane to the bizarre, and they often carry a unique blend of irony, sarcasm, and dark humor characteristic of 4chan\\'s community.\\nGenerate a python dict  containing 4chan greentexts like so:\\n\\n{{0:\">Wake up early\\n>Watch the sunrise\\n>Feel at peace\\n>Start the day right\\',\\n1:\">Visit grandparents\\n>Listen to their stories\\n>Cherish their wisdom\\n>Grandparents are precious\",\\n2:\">Plant a garden\\n>Watch it bloom\\n>Connect with nature\\\\>Earth smiles back\"}}\\n\\nOnly answer with a valid json file. Use only single-quotes inside the strings.  \\n Your generation should be of the theme: Memes and Internet Humor: Stories that reference popular memes, internet jokes, or trends.\\nGenerate a json with 25 original, hilarious examples.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chanStyleTypes = [\n",
    "    \"Personal Anecdotes: Stories about users' personal experiences, often with unexpected twists or humorous conclusions.\",\n",
    "    \"Historical and Cultural References: Stories that play on historical events, cultural phenomena, or popular media, often with an ironic or satirical twist.\",\n",
    "    \"Fantasy and Sci-Fi: Tales involving elements from fantasy or science fiction, such as time travel, mythical creatures, or futuristic scenarios.\",\n",
    "    \"Everyday Observations: Relatable incidents or observations from daily life, presented in a humorous or exaggerated manner.\",\n",
    "    \"Work and School Experiences: Stories about interesting, funny, or bizarre occurrences in professional or educational settings.\",\n",
    "    \"Relationships and Social Interactions: Accounts of romantic endeavors, friendships, family dynamics, and awkward social situations.\",\n",
    "    \"Internet and Technology: Anecdotes about experiences with technology, internet culture, and digital interactions.\",\n",
    "    \"Memes and Internet Humor: Stories that reference popular memes, internet jokes, or trends.\",\n",
    "    \"Philosophical and Pseudo-Intellectual Musings: Abstract, often humorous musings on life, existence, and philosophical concepts.\",\n",
    "    \"Parody and Satire: Stories that mock or satirize various aspects of society, politics, celebrity culture, etc.\",\n",
    "    \"Adventure and Exploration: Imaginary or exaggerated tales of adventure, exploration, or extraordinary experiences.\",\n",
    "    \"Role Reversal and Perspective Shifts: Stories told from unconventional perspectives or featuring surprising role reversals.\"\n",
    "]\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Greentexts on 4chan cover a wide range of topics, reflecting the diverse interests and experiences of its user base.\n",
    "These topics can range from the mundane to the bizarre, and they often carry a unique blend of irony, sarcasm, and dark humor characteristic of 4chan's community.\n",
    "Generate a python dict  containing 4chan greentexts like so:\n",
    "\n",
    "{{0:\">Wake up early\\n>Watch the sunrise\\n>Feel at peace\\n>Start the day right',\n",
    "1:\">Visit grandparents\\n>Listen to their stories\\n>Cherish their wisdom\\n>Grandparents are precious\",\n",
    "2:\">Plant a garden\\n>Watch it bloom\\n>Connect with nature\\>Earth smiles back\"}}\n",
    "\n",
    "Only answer with a valid json file. Use only single-quotes inside the strings.  \n",
    "\"\"\"\n",
    "def generate_prompt():\n",
    "    prompt = f\"\"\"{system_prompt} Your generation should be of the theme: {chanStyleTypes[np.random.randint(len(chanStyleTypes))]}\\nGenerate a json with 25 original, hilarious examples.\"\"\"\n",
    "    return prompt\n",
    "generate_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_filename(length=12, extension=\".json\"):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(characters) for _ in range(length)) + extension\n",
    "\n",
    "def save_string_to_json(content, directory):\n",
    "    filename = generate_random_filename()\n",
    "    with open(os.path.join(directory, filename), 'w') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_openai_json_format(json_string):\n",
    "    parsed_json_string = json_string.replace(\"{\\n\", \"{\").replace(\"{ \\n\",\"{\").replace(\",\\n\",\",\").replace(\"\\n}\",\"}\").replace(\"\\n }\",\"}\").replace(\"}\\n\",\"}\").replace(\",}\", \"}\").replace(\"\\n\", \"\\\\n\")\n",
    "    return parsed_json_string\n",
    "\n",
    "def make_openai_request(api_key, prompt):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\")\n",
    "    return handle_openai_json_format(response.choices[0].message.content)\n",
    "\n",
    "def save_openai_response_to_file(response, directory):\n",
    "    save_string_to_json(response, directory=directory)\n",
    "\n",
    "def make_and_save_request(api_key, prompt, output_directory=\"./output/\"):\n",
    "    response = make_openai_request(api_key, prompt)\n",
    "    save_openai_response_to_file(response, directory=f\"{output_directory}/\")\n",
    "\n",
    "def get_openai_data(api_key, num_requests=2, output_directory=\"./output/\"):\n",
    "    prompts = [generate_prompt() for _ in range(num_requests)]\n",
    "\n",
    "    Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(make_and_save_request)(api_key, prompt) for prompt in prompts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed: 20.5min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 22.2min\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 27.7min\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed: 29.8min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 529 tasks      | elapsed: 33.6min\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 38.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 40.3min\n",
      "[Parallel(n_jobs=-1)]: Done 669 tasks      | elapsed: 42.8min\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed: 44.9min\n",
      "[Parallel(n_jobs=-1)]: Done 745 tasks      | elapsed: 47.3min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 49.7min\n",
      "[Parallel(n_jobs=-1)]: Done 825 tasks      | elapsed: 52.2min\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed: 54.5min\n",
      "[Parallel(n_jobs=-1)]: Done 909 tasks      | elapsed: 57.0min\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed: 59.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 62.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1042 tasks      | elapsed: 64.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed: 67.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed: 70.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1185 tasks      | elapsed: 73.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 76.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1285 tasks      | elapsed: 79.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1336 tasks      | elapsed: 82.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1389 tasks      | elapsed: 86.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 89.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed: 92.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed: 96.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1609 tasks      | elapsed: 99.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1666 tasks      | elapsed: 103.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1725 tasks      | elapsed: 106.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 109.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1845 tasks      | elapsed: 113.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1906 tasks      | elapsed: 117.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 120.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 124.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed: 128.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2162 tasks      | elapsed: 132.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2229 tasks      | elapsed: 136.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2296 tasks      | elapsed: 140.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2365 tasks      | elapsed: 144.3min\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A result has failed to un-serialize. Please ensure that the objects returned by the function are always picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/maxmynter/Library/Caches/pypoetry/virtualenvs/wholesomegreentextllm-RqUaalAq-py3.10/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 661, in wait_result_broken_or_wakeup\n    result_item = result_reader.recv()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py\", line 256, in recv\n    return _ForkingPickler.loads(buf.getbuffer())\nTypeError: APIStatusError.__init__() missing 2 required keyword-only arguments: 'response' and 'body'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_openai_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_requests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mget_openai_data\u001b[0;34m(api_key, num_requests, output_directory)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_openai_data\u001b[39m(api_key, num_requests\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, output_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     25\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m [generate_prompt() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_requests)]\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_and_save_request\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wholesomegreentextllm-RqUaalAq-py3.10/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wholesomegreentextllm-RqUaalAq-py3.10/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wholesomegreentextllm-RqUaalAq-py3.10/lib/python3.10/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1693\u001b[0m \n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wholesomegreentextllm-RqUaalAq-py3.10/lib/python3.10/site-packages/joblib/parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wholesomegreentextllm-RqUaalAq-py3.10/lib/python3.10/site-packages/joblib/parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wholesomegreentextllm-RqUaalAq-py3.10/lib/python3.10/site-packages/joblib/parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A result has failed to un-serialize. Please ensure that the objects returned by the function are always picklable."
     ]
    }
   ],
   "source": [
    "\n",
    "get_openai_data(openai_api_key, num_requests=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wholesomegreentextllm-RqUaalAq-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
